import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import time

# â¬…ï¸ MUSI BYÄ† PIERWSZA
st.set_page_config(page_title="DEBUG: analiza kontekstowa", layout="centered")

# MODEL
@st.cache_resource
def load_model():
    return SentenceTransformer('paraphrase-MiniLM-L6-v2')

model = load_model()

# INTERFEJS
st.title("ğŸ Debug: analiza semantyczna witryn")

st.header("ğŸ¯ WprowadÅº dane wejÅ›ciowe:")
product = st.text_area("ğŸ›ï¸ Opis produktu", "Ekskluzywne zegarki dla mÄ™Å¼czyzn")
audience = st.text_area("ğŸ‘¥ Grupa docelowa", "ZamoÅ¼ni mÄ™Å¼czyÅºni 35+, zainteresowani modÄ… i prestiÅ¼em")

# Wczytywanie listy domen z pliku
def load_sites_from_file(file_path="sites.txt"):
    try:
        with open(file_path, "r") as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        st.error("âŒ Nie znaleziono pliku sites.txt w repozytorium.")
        return []

# ÅšcieÅ¼ka z maks. 2 segmentami
def is_allowed_path_depth(url):
    path = urlparse(url).path
    segments = [seg for seg in path.split("/") if seg]
    return len(segments) <= 2

# Pobieramy linki wewnÄ™trzne z domeny
def extract_links(base_url, limit=30):
    try:
        page = requests.get(base_url, timeout=5)
        soup = BeautifulSoup(page.text, "html.parser")
        links = set()
        for tag in soup.find_all("a", href=True):
            href = tag["href"]
            full_url = urljoin(base_url, href)
            if urlparse(full_url).netloc == urlparse(base_url).netloc:
                if is_allowed_path_depth(full_url):
                    links.add(full_url)
            if len(links) >= limit:
                break
        return list(links)
    except Exception as e:
        print(f"BÅ‚Ä…d przy pobieraniu linkÃ³w z {base_url}: {e}")
        return []

# Pobieranie tekstu
def extract_text(url):
    try:
        page = requests.get(url, timeout=5)
        soup = BeautifulSoup(page.text, "html.parser")
        return ' '.join(soup.stripped_strings)[:3000]
    except Exception as e:
        print(f"BÅ‚Ä…d przy pobieraniu treÅ›ci z {url}: {e}")
        return ""

# ANALIZA
if st.button("ğŸš€ Uruchom analizÄ™ debugujÄ…cÄ…"):
    if not product or not audience:
        st.warning("â— WprowadÅº opis produktu i grupy docelowej.")
        st.stop()

    st.info("ğŸ“‚ Wczytywanie domen z `sites.txt`...")
    sites = load_sites_from_file()
    if not sites:
        st.warning("Brak domen.")
        st.stop()

    query_embedding = model.encode(product + " " + audience, convert_to_tensor=True)
    results = []

    st.info("ğŸ”¬ Analiza trwa... obserwuj logi poniÅ¼ej.")
    progress = st.progress(0)
    eta_placeholder = st.empty()

    total = len(sites)
    start_time = time.time()
    real_scans = 0

    for idx, site in enumerate(sites):
        print(f"\nğŸŒ Domena: {site}")
        links = extract_links(site, limit=30)
        best_score = 0.0
        best_url = ""
        found_texts = 0

        for link in links:
            print(f"â†’ Link: {link}")
            text = extract_text(link)
            print(f"   ğŸ“„ DÅ‚ugoÅ›Ä‡ tekstu: {len(text)}")

            if text.strip():
                found_texts += 1
                page_embedding = model.encode(text, convert_to_tensor=True)
                score = util.pytorch_cos_sim(query_embedding, page_embedding).item()
                print(f"   âœ… Score: {score:.4f}")
                if score > best_score:
                    best_score = score
                    best_url = link
            else:
                print(f"   âš ï¸ Pusta treÅ›Ä‡ lub niedostÄ™pna")

            time.sleep(0.5)

        if found_texts > 0:
            real_scans += 1

        results.append({
            "Domena": site,
            "Najlepiej dopasowana podstrona": best_url,
            "Dopasowanie (%)": round(best_score * 100, 2),
            "IloÅ›Ä‡ przeanalizowanych linkÃ³w": found_texts
        })

        # PostÄ™p i ETA
        progress.progress((idx + 1) / total)
        elapsed = time.time() - start_time
        avg_time = elapsed / (idx + 1)
        remaining = avg_time * (total - idx - 1)
        mins, secs = divmod(int(remaining), 60)
        eta_placeholder.info(f"â±ï¸ Szacowany czas do koÅ„ca: {mins} min {secs} sek")

    df = pd.DataFrame(sorted(results, key=lambda x: -x["Dopasowanie (%)"]))
    st.success(f"âœ… Gotowe. Przeanalizowano {real_scans}/{len(sites)} domen z treÅ›ciÄ….")
    st.dataframe(df)
    st.download_button("ğŸ“¥ Pobierz CSV", df.to_csv(index=False), file_name="debug_ranking.csv")
